---

## üß† Overview

I used the Titanic dataset to predict survival while incorporating domain knowledge into the model. Monotonic constraints ensure that the model‚Äôs predictions follow logical assumptions (e.g., wealthier passengers likely had better survival chances). This improves interpretability and aligns the model with real-world reasoning.

---

## üõ†Ô∏è Step-by-Step Explanation

### 1. Loading the Dataset
- Loaded the Titanic dataset from a CSV file using `pandas.read_csv`.
- The dataset includes features like `Pclass`, `Sex`, `Age`, `SibSp`, `Parch`, `Fare`, and `Embarked`.

### 2. Handling Missing Values
- Filled missing values in numeric columns (`Age`, `Fare`) with the mean.
- Filled missing values in the categorical column (`Embarked`) with the most frequent value (`'S'`).

### 3. Encoding Categorical Variables
- Encoded `Sex` and `Embarked` into numeric values using `LabelEncoder`.
- Ensures compatibility with XGBoost, which requires numeric input.

### 4. Splitting the Dataset
- Split the data into training (80%) and validation (20%) sets to evaluate generalization.

### 5. Converting Data to DMatrix
- Converted the datasets into XGBoost‚Äôs optimized `DMatrix` format for faster computations.

### 6. Defining Monotonic Constraints
- Enforced a **positive monotonic constraint** on the `Fare` feature (index 5 in the DMatrix).
- Constraint tuple: `(0, 0, 0, 0, 0, 1, 0)` ‚Äî corresponds to:
